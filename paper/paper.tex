\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{document}

\mainmatter

\title{Fast Nearest Neighbour Classification}
\subtitle{\textnormal{\small{Seminar\\
Intelligente Software-Systeme\\
Sommersemester 2015\\\vspace{1\baselineskip}
Stefan Fricke\\\vspace{2\baselineskip}
May 2016\\
Supervisor: Stephan Spiegel\\\vspace{1\baselineskip}}}}

\titlerunning{Fast Nearest Neighbour Classification}

\author{Gordon Lesti\\313249\\Studiengang: Informatik\\gordon.lesti@campus.tu-berlin.de\\\vspace{5\baselineskip}}

\authorrunning{Fast Nearest Neighbour Classification}

\institute{Technische Universit\"at Berlin\\
Fakult\"at IV Elektrotechnik und Informatik\\
Fachgebiet AOT\\
Prof. Dr. Sahin Albayrak\\
\url{http://www.aot.tu-berlin.de/}}

\toctitle{Fast Nearest Neighbour Classification}
\tocauthor{Gordon Lesti}
\maketitle
\pagenumbering{roman}

\addcontentsline{toc}{section}{Abstract}
\begin{abstract}
This term paper is about nearest neighbour classifiers which work with the triangle inequality of metric spaces. The
Orchard’s Algorithm, the Annulus Method and the AESA will be explained on examples and compared to the Full Search.
Benchmarks will show the advantages and disadvantages of the three algorithms. Counting the calls of the distance
function is the focus of the benchmarks.
\keywords{Nearest neighbour classifiers, Triangle inequality, Metric space}
\end{abstract}

\tableofcontents
\addcontentsline{toc}{section}{Table of Contents}

\newpage
\pagenumbering{arabic}

\section{Introduction}

Kenneth L. Clarkson wrote the paper \textit{Nearest-Neighbor Searching and Metric Space Dimensions}[1] which deals
amongst other approaches with the topic nearest neighbour search algorithms that use the triangle inequality. This paper
focuses on three of them.

\subsection{Description of the problem}

The mission of a nearest neighbour search algorithm is to find in a given set of items, that item with the smallest
distance to a given query item.\\
The requirements are:
\begin{itemize}
	\item a set $\mathbb{U}$
	\item a distance function $D$ on $\mathbb{U}$, with $d: \mathbb{U} \times \mathbb{U} \to \mathbb{R}$
	\item a set $S \subset \mathbb{U}$ with size $n$
	\item a query $q \in \mathbb{U}$
\end{itemize}
The solution is:
\begin{itemize}
	\item an $a \in S$, with $D(q, a) \le D(q, x)$ for all $x \in S$
\end{itemize}

\section{Algorithms}

Every nearest neighbour search algorithm can be divided into two steps.
\paragraph{Preprocessing}
During the preprocessing the algorithm prepares the given set $\mathbb{S}$ for its needs.

\paragraph{Query Processing}
The query processing is the main nearest neighbour search, the algorithm takes the query $q$ as input and works on the
preprocessed data to find the nearest neighbour $a$.\\

The following section will explain the preprocessing and query processing of the Full Search, the Orchard’s Algorithm,
the Annulus Method and the AESA.

\subsection{Full Search}

The Full Search is the upper bound for the following algorithms. Exactly $n$ calls of the distance function are required
to find the nearest neighbour in a given $S$ of size $n$.\\

\begin{algorithm}[H]
	\caption{Full Search}
	\label{fullsearch}
	\begin{algorithmic}
		\REQUIRE query $q \in \mathbb{U}$, set $S \subset \mathbb{U}$ of size $n$, distance function $D$
		\ENSURE $a \in S$ with $D(q, a) \le D(q, x)$ $\forall x \in S$
		\STATE $r := \infty$
		\STATE $a :=$ NULL
		\FORALL{$x \in S$}
			\STATE $xd := D(x, q)$
			\IF{$sd < r$}
				\STATE $r := xd$
				\STATE $a := x$
			\ENDIF
		\ENDFOR
		\RETURN $a$
	\end{algorithmic}
\end{algorithm}

The advantage compared to the following algorithms is, that the Full Search works also in not metric spaces. The Full
Search does not need a preprocessing.

\subsection{Orchard’s Algorithm}

\paragraph{Preprocessing}

The preprocessing of the Orchard’s Algorithm has a high complexity. For every $p \in S$, the algorithm will generate a
list that contains all $x \in S\setminus\left\{ {p}\right\}$ with their distance $D(p, x)$. Those lists will be ordered
ascending to the distance. For a set $S$ with size $n$, the preprocessing will call the distance function
$\frac{n(n-1)}{2}$ times and will sort $n$ lists of size $n-1$.

\paragraph{Query Processing}

When calling the query processing with query $q$, the algorithm will randomly select one item $c \in S$ as initial
candidate and calculate $D(c, q)$. Afterwards the algorithm walks over the ordered list of $c$ and with every picked $s$
from the list the algorithm calculates $D(l, q)$. Item $l$ is the new candidate $c$ if $D(l, q)$ is smaller than
$D(c, q)$. Thereafter the algorithm walks over the list of $l$. The algorithm interupts if it reaches the end of a list
or if $D(c, s) > 2D(c, q)$. The current candidate $c$ is the nearest neighbour.

\subsubsection{Orchard’s Algorithm with marked bits}

There is a improved version of the Orchard’s Algorithm to ensure that no distance between an item $x \in S$ and $q$
is calculated twice. This can be achieved by boolean flags for every item during query processing of the Orchard’s
Algorithm.

\subsection{Annulus Method}

\paragraph{Preprocessing}

Instead of generating a list for all $x \in S$, the Annulus Method generates just one list for a random $p^* \in S$.
During preprocessing the algorithm generates one list which contains all $x \in S$ with their distance $D(p^*, x)$. This
list will be ordered ascending to the distance. For a set $S$ with size $n$, the preprocessing will call the distance
function $n$ times and will sort one list of size $n$.

\paragraph{Query Processing}

The query processing of the Annulus Method starts by picking a random candidate $c \in S$ from the ordered list of
$p^*$. Afterwards the algorithm walks alternating away from $p^*$ and back to it in the list. If the current item $s$
has $D(s, q) < D(c, q)$, the algorithm sets $s$ as the new candidate $c$. If the current item $s$ is under $c$ in the
list and $D(p^*, s) < D(p^*, q) - D(c, q)$, all items under $s$ in the list will be ignored. If the current
item $s$ is above $c$ in the list and $D(p^*, s) > D(p^*, q) + D(c, q)$, the algorithm will ignore all items above
$s$ in the list. The item $c$ is the nearest neighbour, if the entire list is traversed.

\subsection{AESA}

\paragraph{Preprocessing}

The preprocessing for the AESA is starting the same way as the preprocessing of the Orchard’s Algorithm. The algorithm
calculates the distance $D(x, y)$ of every $x \in S$ to every $y \in S\setminus\left\{ {x}\right\}$. But instead of
writing those distances into lists and sorting them, the algorithm writes those distances into a matrix to access them
quickly. For a set $S$ with size $n$, the preprocessing will call the distance function $\frac{n(n-1)}{2}$ times and
will sort $n$ lists of size $n-1$.

\paragraph{Query Processing}

When starting with the query processing, the algorithm generates a list for all items $x \in S$ with a bounded
distance $D_P(x, q) = -\infty$ and initialize $r = \infty$ as smallest distance to $q$. Afterwards the algorithm
iterates the following steps until the list is empty.
\begin{itemize}
	\item pick and remove $x$ from the list with the smallest $D_p(x, q)$ or a random one during first iteration
	\item calculate $D(x, q)$
	\item set $r = D(x, q)$ if $r > D(x, q)$ and remember $x$ as current nearest neighbour
	\item for all $y$ from the list, set $D_P(y, q) = max(D_P(y, q), |D(x, q) - D(x, y)|)$ and remove $y$ from the list
		if the new $D_P(y, q) > r$
\end{itemize}

\section{Benchmarks}

As described above, the following benchmarks will not compare the runtime of the mentioned algorithms. The benchmarks
will compare the calls of the distance function that the algorithm needs for preprocessing and for finding a nearest
neighbour. Those benchmarks are useful when the distance function has a heavy complexity. For example the string
metric Levenshtein distance or in higher dimensional vector spaces.\\
In general a benchmark is affected by the implementation, by the programming language, by the executing machine, by the
domain of the problem and the distribution of the example. As the following benchmarks only counting the calls of the
distance function its are not affected by the programming language or the executing machine.

\subsection{Implementation}

The algorithms above are all implemented in Java and can be found in the following git
repository\footnote[1]{\url{https://github.com/GordonLesti/FastNearestNeighbourClassification}}.\\
Since the benchmarks are independed from the programming language and the executing machine, the implementation of the
algorithms uses sometimes not the most effective data structures. But that never effects the logical flow of the
algorithms.\\

A nearest neighbour algorithm implemented in Java should handle every type of objects that a set $\mathbb{U}$ can
represent under the condition that there is a metric on $\mathbb{U}$. Because of that, a nearest neighbour algorithm
class needs to extend from the abstract class\\ \verb+FastNearestNeighbourClassificator<T, D extends Comparable<D>>+
that\\ can be found in the package \verb+fnnc.algo+. The generic \verb+T+ represents an object from the set $\mathbb{U}$
and the generic \verb+D+ represents the distance and has to be of type \verb+Comparable+ from the package
\verb+java.lang+.\\

A possible example for the type \verb+T+ can be \verb+Point2D.Double+ from the package
\verb+java.awt.geom+ and a possible way to implement a distance function would be the euclidean distance of type
\verb+Double+.

\subsubsection{FastNearestNeighbourClassificator}

The abstract class\\ \verb+FastNearestNeighbourClassificator<T, D extends Comparable<D>>+ has\\ two protected class
variables:
\begin{itemize}
	\item a \verb+distanceCalculator+ of type \verb+DistanceCalculator<T, D>+
	\item a \verb+objectCollection+ of type \verb+Collection<T>+
\end{itemize}
and three simple class methods:
\begin{itemize}
	\item a constructor that takes an object of type \verb+DistanceCalculator<T, D>+ from the package
		\verb+fnnc.model+ and sets the protected class variable
	\item a default function \verb+preProcessing+ that takes a \verb+Collection<T>+ from the package
		\verb+java.util+ and sets the protected class variable
	\item a abstract function \verb+calculateNearestNeighbour+ that takes the query object of type \verb+T+ and
		returns the nearest neighbour object of type \verb+T+
\end{itemize}

An instance of type \verb+FastNearestNeighbourClassificator+ should not be allowed and can not calculate the distance
between two objects. So that an instance of \verb+DistanceCalculator+ should be injected into the constructor. All
implemented algorithms exclude the \verb+FullSearch+ require that the\\ \verb+DistanceCalculator+ returns a distance of
type \verb+Double+ instead of only\\ \verb+Comparable+. This is connected with the fact that only the Full Search can
operate in a not metric space.

\subsubsection{DistanceCalculator}

The abstract class\\ \verb+DistanceCalculator<Point2D.Double, Double>+ has just one private \verb+count+ of type
\verb+int+ and three class methods:
\begin{itemize}
	\item a public final function \verb+calculateDistance+ that takes two objects of type \verb+T+, increases the
		internal count and returns the result of\\ \verb+internalCalculateDistance+
	\item a public final function \verb+reset+ that resets the internal count
	\item a protected abstract function \verb+internalCalculateDistance+ that takes two objects of type \verb+T+ and
		returns the distance object of type \verb+D+
\end{itemize}

\subsection{Example data}

The benchmarks will use a \verb+java.util.Collection+ of type\\ \verb+java.awt.geom.Point2D+ that are representing
points in $\mathbb{R}^2$. For every size $n \in \left\{ {100, 200, \dots, 1900, 2000}\right\}$, every algorithm will get
thousand sets of size $n$ with randomly generated points. The class\\ \verb+fnnc.model.point2ddouble.DistanceCalculator+
will count every call of the function \verb+internalCalculateDistance+ and after all the application will calculate the
average of the calls for every algorithm.

\subsection{Result}

\subsubsection{Preprocessing}

The results of the preprocessing are static. Figure~\ref{fig:preprocessing:tabular} and
figure~\ref{fig:preprocessing:graph} summarizing again the complexity of the preprocessing for every algorithm
depending on the size of set $S$.

\begin{figure}
	\begin{center}
		\begin{tabular}{| l | c | c | c | c | c |}
			\hline
		  	& Full Search & Orchard & Orchard MarkBits & Annulus & AESA \\ \hline
		  	$n$ & 0 & $\frac{n(n-1)}{2}$ & $\frac{n(n-1)}{2}$ & $n$ & $\frac{n(n-1)}{2}$ \\ \hline
		\end{tabular}
	\end{center}
	\caption{The calls of the distance function $D$ during preprocessing depending on the size $n$ of the test data set
		$S$.}
	\label{fig:preprocessing:tabular}
\end{figure}

\begin{figure}
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				ymin=0,
				xmin=0,
				xmax=2000,
				xlabel=size of test data set $S$,
				ylabel=calls of distance function $D$,
				legend pos=outer north east]
				\addplot table {data/preprocessing/fullsearch.csv};
	            \addlegendentry{Full Search}
	            \addplot table {data/preprocessing/orchards.csv};
	            \addlegendentry{Orchard}
				\addplot table {data/preprocessing/orchardsmb.csv};
				\addlegendentry{Orchard MarkBits}
	            \addplot table {data/preprocessing/annulus.csv};
	            \addlegendentry{Annulus}
	            \addplot table {data/preprocessing/aesa.csv};
	            \addlegendentry{AESA}
			\end{axis}
		\end{tikzpicture}
	\end{center}
	\caption{The average calls of the distance function $D$ during preprocessing depending on the size of the test data
		set $S$ visualized with graphs.}
	\label{fig:preprocessing:graph}
\end{figure}

\subsubsection{Query Processing}

Figure~\ref{fig:queryprocessing:tabular} and figure~\ref{fig:queryprocessing:graph} show the results of the query
processing for all algorithms.

\begin{figure}
	\begin{center}
		\begin{tabular}{| l | c | c | c | c | c |}
			\hline
		  	& Full Search & Orchard & Orchard MarkBits & Annulus & AESA \\ \hline
		  	100 & 100 & 29 & 20 & 46 & 4 \\ \hline
			200 & 200 & 39 &26 & 82 & 4 \\ \hline
			300 & 300 & 46 & 30 & 117 & 4 \\ \hline
			400 & 400 & 53 & 35 & 160 & 4 \\ \hline
			500 & 500 & 57 & 40 & 189 & 4 \\ \hline
			600 & 600 & 63 & 42 & 228 & 4 \\ \hline
			700 & 700 & 68 & 44 & 263 & 4 \\ \hline
			800 & 800 & 73 & 46 & 303 & 4 \\ \hline
			900 & 900 & 78 & 50 & 341 & 4 \\ \hline
			1000 & 1000 & 79 & 51 & 367 & 4 \\ \hline
			1100 & 1100 & 84 & 56 & 409 & 4 \\ \hline
			1200 & 1200 & 86 & 56 & 442 & 4 \\ \hline
			1300 & 1300 & 89 & 59 & 466 & 4 \\ \hline
			1400 & 1400 & 90 & 60 & 510 & 4 \\ \hline
			1500 & 1500 & 97 & 63 & 541 & 4 \\ \hline
			1600 & 1600 & 100 & 66 & 573 & 4 \\ \hline
			1700 & 1700 & 102 & 67 & 624 & 4 \\ \hline
			1800 & 1800 & 104 & 69 & 660 & 4 \\ \hline
			1900 & 1900 & 109 & 71 & 664 & 4 \\ \hline
			2000 & 2000 & 113 & 73 & 740 & 4 \\ \hline
		\end{tabular}
	\end{center}
	\caption{The average calls of the distance function $D$ during query processing depending on the size of the test
		data set $S$.}
	\label{fig:queryprocessing:tabular}
\end{figure}

\begin{figure}
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				ymin=0,
				xmin=0,
				xmax=2000,
				xlabel=size of test data set $S$,
				ylabel=average calls of distance function $D$,
				legend pos=outer north east]
				\addplot table {data/queryprocessing/fullsearch.csv};
	            \addlegendentry{Full Search}
	            \addplot table {data/queryprocessing/orchards.csv};
	            \addlegendentry{Orchard}
				\addplot table {data/queryprocessing/orchardsmb.csv};
				\addlegendentry{Orchard MarkBits}
	            \addplot table {data/queryprocessing/annulus.csv};
	            \addlegendentry{Annulus}
	            \addplot table {data/queryprocessing/aesa.csv};
	            \addlegendentry{AESA}
			\end{axis}
		\end{tikzpicture}
	\end{center}
	\caption{The average calls of the distance function $D$ during query processing depending on the size of the test
		data set $S$ visualized with graphs.}
	\label{fig:queryprocessing:graph}
\end{figure}

\paragraph{Standard deviation}

Some of the algorithms needed more and more tries to stabilize the average result of the query processing.
Figure~\ref{fig:standarddeviation:tabular} and figure~\ref{fig:standarddeviation:graph} show the standard deviation of
the results for every algorithm depending on the size of set $S$.

\begin{figure}
	\begin{center}
		\begin{tabular}{| l | c | c | c | c | c |}
			\hline
		  	& Full Search & Orchard & Orchard MarkBits & Annulus & AESA \\ \hline
		  	100 & 0 & 13 & 8 & 23 & 1 \\ \hline
			200 & 0 & 17 & 11 & 46 & 1 \\ \hline
			300 & 0 & 21 & 13 & 69 & 1 \\ \hline
			400 & 0 & 25 & 15 & 94 & 1 \\ \hline
			500 & 0 & 27 & 17 & 119 & 1 \\ \hline
			600 & 0 & 29 & 19 & 144 & 1 \\ \hline
			700 & 0 & 32 & 19 & 162 & 1 \\ \hline
			800 & 0 & 33 & 21 & 190 & 1 \\ \hline
			900 & 0 & 36 & 22 & 213 & 1 \\ \hline
			1000 & 0 & 37 & 23 & 232 & 1 \\ \hline
			1100 & 0 & 38 & 24 & 264 & 1 \\ \hline
			1200 & 0 & 40 & 26 & 277 & 1 \\ \hline
			1300 & 0 & 43 & 27 & 301 & 1 \\ \hline
			1400 & 0 & 43 & 27 & 339 & 1 \\ \hline
			1500 & 0 & 47 & 27 & 350 & 1 \\ \hline
			1600 & 0 & 46 & 29 & 374 & 1 \\ \hline
			1700 & 0 & 48 & 29 & 401 & 1 \\ \hline
			1800 & 0 & 49 & 31 & 429 & 1 \\ \hline
			1900 & 0 & 52 & 32 & 442 & 1 \\ \hline
			2000 & 0 & 52 & 33 & 487 & 1 \\ \hline
		\end{tabular}
	\end{center}
	\caption{The standard deviation of the calls of the distance function $D$ during query processing depending on the
		size of the test data set $S$.}
	\label{fig:standarddeviation:tabular}
\end{figure}

\begin{figure}
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				ymin=0,
				xmin=0,
				xmax=2000,
				xlabel=size of test data set $S$,
				ylabel=standard deviation of calls of distance function $D$,
				legend pos=outer north east]
				\addplot table {data/standarddeviation/fullsearch.csv};
	            \addlegendentry{Full Search}
	            \addplot table {data/standarddeviation/orchards.csv};
	            \addlegendentry{Orchard}
				\addplot table {data/standarddeviation/orchardsmb.csv};
				\addlegendentry{Orchard MarkBits}
	            \addplot table {data/standarddeviation/annulus.csv};
	            \addlegendentry{Annulus}
	            \addplot table {data/standarddeviation/aesa.csv};
	            \addlegendentry{AESA}
			\end{axis}
		\end{tikzpicture}
	\end{center}
	\caption{The standard deviation of the calls of the distance function $D$ during query processing depending on the
		size of the test data set $S$ visualized with graphs.}
	\label{fig:standarddeviation:graph}
\end{figure}

\section{Conclusions}

\paragraph{Full Search} The results may leave the impression that the Full Search is not a good choice. But this
algorithm remains useful if the set $S$ changes completely with every new query or when $\mathbb{U}$ is no metric space.

\paragraph{Annulus Method} The Annulus Method has a very complex query processing. But on a frequently changing set $S$
a affordable preprocessing. Other algorithms with more complex preprocessing maybe hit its limits on huge sets that
can be handled by the Annulus Method. The Annulus Method was the algorithm with the highest standard deviation during
the benchmarks.

\paragraph{Orchard’s Algorithm with and without marking bits} During query processing the Orchard’s Algorithm with
marking bits consumed on average only 66\% of the distance function calls that the algorithm needed without marking
bits.

\paragraph{AESA} Depending on the benchmarks above, the AESA should be preferred instead of the Orchard’s Algorithm.
Both algorithms have a very complex preprocessing that may not work on very huge sets. But the AESA consumes more less
calls of the distance function. The impact on real world applications should be tested. Beside the Full Search the AESA
was the most stable algorithm.

\section{Future work}

The LAESA algorithm and Metric Trees are missing in the benchmarks above. Both approaches are promising and their
results compared to algorithms of this paper would be interesting.

\begin{thebibliography}{4}
	\bibitem{url} Kenneth L. Clarkson, \"Nearest-Neighbor Searching and Metric Space Dimensions\" (October 2005), \url{http://kenclarkson.org/nn_survey/p.pdf}
\end{thebibliography}

\end{document}
